{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==2.3.5\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n",
    "!pip install pdfminer\n",
    "\n",
    "!pip install -U transformers==3.0.0\n",
    "!python -m nltk.downloader punkt\n",
    "!git clone https://github.com/patil-suraj/question_generation.git\n",
    "%cd question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "def parse_10q_filing(link, section):\n",
    "    \n",
    "    if section not in [0, 1, 2, 3]:\n",
    "        print(\"Not a valid section\")\n",
    "        sys.exit()\n",
    "    \n",
    "    def get_text(link):\n",
    "        page = requests.get(link, headers={'User-Agent': 'Mozilla'})\n",
    "        html = bs(page.content, \"lxml\")\n",
    "        text = html.get_text()\n",
    "        text = unicodedata.normalize(\"NFKD\", text).encode('ascii', 'ignore').decode('utf8')\n",
    "        text = text.split(\"\\n\")\n",
    "        text = \" \".join(text)\n",
    "        return(text)\n",
    "    \n",
    "    def extract_text(text, item_start, item_end):\n",
    "        item_start = item_start\n",
    "        item_end = item_end\n",
    "        starts = [i.start() for i in item_start.finditer(text)]\n",
    "        ends = [i.start() for i in item_end.finditer(text)]\n",
    "        positions = list()\n",
    "        for s in starts:\n",
    "            control = 0\n",
    "            for e in ends:\n",
    "                if control == 0:\n",
    "                    if s < e:\n",
    "                        control = 1\n",
    "                        positions.append([s,e])\n",
    "        item_length = 0\n",
    "        item_position = list()\n",
    "        for p in positions:\n",
    "            if (p[1]-p[0]) > item_length:\n",
    "                item_length = p[1]-p[0]\n",
    "                item_position = p\n",
    "\n",
    "        item_text = text[item_position[0]:item_position[1]]\n",
    "\n",
    "        return(item_text)\n",
    "\n",
    "    text = get_text(link)\n",
    "        \n",
    "    if section == 1 or section == 0:\n",
    "        try:\n",
    "            item1_start = re.compile(\"item\\s*[1][\\.\\;\\:\\-\\_]*\\s*\\\\bF\", re.IGNORECASE)\n",
    "            item1_end = re.compile(\"item\\s*2[\\.\\;\\:\\-\\_]\\s*Man|item\\s*3[\\.\\,\\;\\:\\-\\_]\\s*Quanti\", re.IGNORECASE)\n",
    "            finText = extract_text(text, item1_start, item1_end)\n",
    "        except:\n",
    "            finText = \"Something went wrong!\"\n",
    "        \n",
    "    if section == 2 or section == 0:\n",
    "        try:\n",
    "            item2_start = re.compile(\"item\\s*[2][\\.\\;\\:\\-\\_]*\\s*\\\\bM\", re.IGNORECASE)\n",
    "            item2_end = re.compile(\"item\\s*3[\\.\\;\\:\\-\\_]\\sQuanti|item\\s*4[\\.\\,\\;\\:\\-\\_]\\s*\", re.IGNORECASE)\n",
    "            mdaText = extract_text(text, item2_start, item2_end)\n",
    "        except:\n",
    "            mdaText = \"Something went wrong!\"\n",
    "            \n",
    "    if section == 3 or section == 0:\n",
    "        try:\n",
    "            item3_start = re.compile(\"item\\s*[3][\\.\\;\\:\\-\\_]*\\s*\\\\bQ\", re.IGNORECASE)\n",
    "            item3_end = re.compile(\"item\\s*4[\\.\\;\\:\\-\\_]\\sCon|item\\s*4[\\.\\,\\;\\:\\-\\_]\\s*\", re.IGNORECASE)\n",
    "            riskText = extract_text(text, item3_start, item3_end)\n",
    "        except:\n",
    "            riskText = \"Something went wrong!\"        \n",
    "    \n",
    "    if section == 0:\n",
    "        data = [finText, mdaText, riskText]\n",
    "    elif section == 1:\n",
    "        data = [finText]\n",
    "    elif section == 2:\n",
    "        data = [mdaText]\n",
    "    elif section == 3:\n",
    "        data = [riskText]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "\n",
    "def parse_10k_filling(filing_url):\n",
    "    api_key = api_key\n",
    "\n",
    "    response = requests.get(f'https://api.sec-api.io/filing-reader?token={api_key}&type=pdf&url={filing_url}')\n",
    "    with open('/content/metadata.pdf', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    output_string = StringIO()\n",
    "    with open('/content/metadata.pdf', 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        \n",
    "        out = []\n",
    "        toc_page = -1\n",
    "        for idx, page in enumerate(PDFPage.create_pages(doc)):\n",
    "            interpreter.process_page(page)\n",
    "            out.append(output_string.getvalue().replace('\\x00',''))\n",
    "            output_string.truncate(0)\n",
    "            if toc_page == -1 and \"table of contents\" in out[-1].lower() and 'item' in out[-1].lower():\n",
    "                toc_page = idx\n",
    "        \n",
    "    if toc_page != -1:\n",
    "        items_list = re.findall(\"item +([0-9]+[a-z]?)\", out[toc_page].lower().replace('\\n',' '))\n",
    "    else:\n",
    "        toc_page = 2\n",
    "\n",
    "    output = '\\n\\n'.join(out)\n",
    "    zero_idx = len('\\n\\n'.join(out[:toc_page+1])) + 2\n",
    "    idx = output[zero_idx:].lower().find('item ' + items_list[0] + '.')\n",
    "    if idx == -1:\n",
    "        idx = output[zero_idx:].lower().find('item ' + items_list[0])\n",
    "    if idx == -1:\n",
    "        idx = output[zero_idx:].lower().find(items_list[0] + '.')\n",
    "\n",
    "    zero_idx += idx\n",
    "\n",
    "    output = output[zero_idx:]\n",
    "\n",
    "    sections_dict = {}\n",
    "    for item in items_list:\n",
    "        sections_dict[str(item)] = ''\n",
    "\n",
    "    last_idx = 0\n",
    "    last_item = items_list[0]\n",
    "\n",
    "    for idx, item in enumerate(items_list[1:]):\n",
    "        cur_output = output[last_idx:].lower()\n",
    "        cur_idx = cur_output.find('item ' + item + '.')\n",
    "        # if cur_idx == -1:\n",
    "        #     cur_idx = cur_output.find('item ' + item)\n",
    "        if cur_idx == -1 and len(re.findall(\"[a-z]+\", item)):\n",
    "            cur_idx = cur_output.find(item + '.')\n",
    "        if cur_idx == -1:\n",
    "            continue\n",
    "        sections_dict[last_item] = output[last_idx : last_idx + cur_idx]\n",
    "        last_idx = last_idx + cur_idx\n",
    "        last_item = item\n",
    "\n",
    "    cur_idx = output[last_idx:].lower().find('signature')\n",
    "    if cur_idx == -1:\n",
    "        sections_dict[last_item] = output[last_idx:]\n",
    "    sections_dict[last_item] = output[last_idx : last_idx + cur_idx]\n",
    "    return sections_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from pipelines import pipeline\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "def extract_metric(text, metric, val_type=\"PERCENT\", k=8):\n",
    "    def preprocess_text(text):\n",
    "        temp_text = text.lower().replace('\\n',' ').replace(' %','%')\n",
    "        return temp_text\n",
    "\n",
    "    def extract_metric_vals(text, val_type=\"PERCENT\", NER=None):\n",
    "        if val_type == \"PERCENT\":\n",
    "            return re.findall(r'(\\d+(?:\\.\\d+)?%?(?!\\S))', text)\n",
    "        if val_type == \"NUMBER\":\n",
    "            return re.findall(r\"[+-]?([0-9]+\\.?[0-9]*|\\.[0-9]+)\", text)\n",
    "        if val_type == \"RATIO\":\n",
    "            return re.findall(r\"([0-9]+:[0-9]+)\", text)\n",
    "        if val_type == \"MONEY\":\n",
    "            values = []\n",
    "            entities = NER(text)\n",
    "            for w in entities.ents:\n",
    "                if w.label_ == 'MONEY':\n",
    "                    values.append(w.text)\n",
    "            return values\n",
    "        return []\n",
    "\n",
    "    # 1. search the metric name in the document\n",
    "    def search_metric(text_list, metric_list):\n",
    "        matched_indices = []\n",
    "        idx = 0\n",
    "        while idx < len(text_list):\n",
    "            if text_list[idx : idx+len(metric_list)] == metric_list:\n",
    "                matched_indices.append(idx)\n",
    "            idx += 1\n",
    "        return matched_indices\n",
    "\n",
    "    # 2. get k words before and after the searched metric\n",
    "    def extract_phrases(text_list, matched_indices, k):\n",
    "        phrases_extracted = []\n",
    "        for idx in matched_indices:\n",
    "            phrase = \"\"\n",
    "            for i in range(-k,k+1):\n",
    "                if idx+i < 0 or idx+i > len(text_list)-1:\n",
    "                    continue\n",
    "                phrase += text_list[idx+i] + \" \"\n",
    "            phrases_extracted.append(phrase)\n",
    "        return phrases_extracted\n",
    "\n",
    "    # 3. apply NER and check for corresonding entity\n",
    "    def find_possible_values(text, metric, NER, k, val_type='PERCENT'):\n",
    "        text = text.replace(',', ' ').replace('-', ' ')\n",
    "        metric = metric.replace('-', ' ')\n",
    "        text_list = text.split(' ')\n",
    "        metric_list = metric.split(' ')\n",
    "        matched_indices = search_metric(text_list, metric_list)\n",
    "        phrases_extracted = extract_phrases(text_list, matched_indices, k)\n",
    "        possible_values = []\n",
    "        for phrase in phrases_extracted:\n",
    "            possible_values += extract_metric_vals(phrase, val_type, NER)\n",
    "        return possible_values\n",
    "\n",
    "    def filter_passage(doc,metric) :\n",
    "        sents = sent_tokenize(doc)\n",
    "        filtered_sents = \".\".join(s for s in sents if metric in s)\n",
    "        return filtered_sents\n",
    "\n",
    "    def get_output(passage,question,metric,NER,val_type='PERCENT') :\n",
    "        tex = preprocess_text(passage)\n",
    "        filtered_passage = filter_passage(tex,metric)\n",
    "        ans = nlp({  \"question\": question,  \"context\": filtered_passage})\n",
    "        ans = ans.replace(',', ' ')\n",
    "        output_values = extract_metric_vals(ans, val_type, NER)\n",
    "        return output_values\n",
    "\n",
    "    def get_correct_value(possible_values, output_values):\n",
    "        correct_values = []\n",
    "        for val1 in output_values:\n",
    "            for val2 in possible_values:\n",
    "                if val1 == val2 and val1 not in correct_values:\n",
    "                    correct_values.append(val1)    \n",
    "        if len(correct_values) > 0:\n",
    "            return correct_values[-1]\n",
    "        return ''\n",
    "\n",
    "    NER = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
    "    nlp = pipeline(\"multitask-qa-qg\")\n",
    "\n",
    "    text = preprocess_text(text)\n",
    "    possible_values = find_possible_values(text, metric, NER, k, val_type)\n",
    "    output_values = get_output(text, f'What is the value of {metric}?', metric, NER, val_type)\n",
    "    correct_value = get_correct_value(possible_values, output_values)\n",
    "    return correct_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_8k_filing(link):\n",
    "    page = requests.get(link, headers={'User-Agent': 'Mozilla'})\n",
    "    html = bs(page.content, \"lxml\")\n",
    "    text = html.get_text()\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode('ascii', 'ignore').decode('utf8')\n",
    "    text = text.split(\"\\n\")\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_api import FullTextSearchApi\n",
    "\n",
    "def get_filings(query, company_cik, startDate='2021-01-01', endDate='2021-12-31'):\n",
    "    fullTextSearchApi = FullTextSearchApi(api_key=api_key)\n",
    "    query = {\n",
    "        \"query\": f'{query}',\n",
    "        \"ciks\": [company_cik],\n",
    "        \"formTypes\": ['8-K', '10-K', '10-Q'],\n",
    "        \"startDate\": startDate,\n",
    "        \"endDate\": endDate,\n",
    "    }\n",
    "    return fullTextSearchApi.get_filings(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"d6215192cce355df08e7cf2eaa66d66a01301931447953f2d6457a5d3d82828f\"\n",
    "company_cik = '0000769397'\n",
    "metric = 'churn rate'\n",
    "val_type = 'MONEY'\n",
    "k = 6\n",
    "\n",
    "filings = get_filings(metric)\n",
    "value = ''\n",
    "for filing in filings['filings']:\n",
    "    if filing['formType'] == '8-K':\n",
    "        text = parse_8k_filing(filing['filingUrl'])\n",
    "        value = extract_metric(text, metric, val_type, k)\n",
    "    elif filing['formType'] == '10-K':\n",
    "        filing_10_k_dict = parse_10k_filling(filing['filingUrl'])\n",
    "        for sec, text in filing_10_k_dict.items():\n",
    "            value = extract_metric(text, metric, val_type, k)\n",
    "            if value:\n",
    "                break\n",
    "    elif filing['formType'] == '10-Q':\n",
    "        filing_10_k_list = parse_10q_filing(filing['filingUrl'],0)\n",
    "        for text in filing_10_k_list:\n",
    "            value = extract_metric(text, metric, val_type, k)\n",
    "            if value:\n",
    "                break\n",
    "    if value:\n",
    "        break\n",
    "\n",
    "print(value)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
